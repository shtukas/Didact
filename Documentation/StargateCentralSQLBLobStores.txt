
Stargate Central SQL BLob Stores

In the beginning we stored each blob independently, at paths of the form

```
[repository]/e3/b0/SHA256-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.data
```

This is perfectly fine as far as the file system is concerned, but I wanted to reduce the backup time, which includes a snapshot and turns out hard-linking that many files does take time.

One way to deal with that problem is to use sqlite files 
    - 01.sqlite3
    - 02.sqlite3
    ...
    - ff.sqlite3

That would be 256 files. The problem is now is that for even moderate data commits, all 256 files are hit and appear as new files on the drives that perform backups or archives. So yeah the backup/archive process is much faster (moving from a million+ files to 256), but huge files (in fact the entire dataset is seen as new at every backup/archive, making the backup process slow again, and making the archive process consume disk space too rapidely).

The solution we need is that files can be used to store data and when they become too big, then they become frozen, and data that would have been put in there is then redirected to other files, until these latter files are too big etc.

For any nhash, we then need a deterministic sequence of file names (or locations) of files to probe sequentially until we find a file small enough to put the data in. The understanding is that the previous files of that sequence are frozen. 

Given the hash

SHA256-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855

the sequence is:
    1. [repository]/e.data
    2. [repository]/e3.data
    3. [repository]/e3b.data
    4. [repository]/e3b0.data
    etc
